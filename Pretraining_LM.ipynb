{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY290rYm-ujl",
        "outputId": "e906dafa-b9c5-4480-ea48-e6c333a1116b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q -U unidecode PyPDF2 beautifulsoup4 regex datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "viC3vOpl_CW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unidecode, random, string, time, pickle, requests, json, csv\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.auto import tqdm\n",
        "from collections import OrderedDict\n",
        "from PyPDF2 import PdfReader\n",
        "import regex as re\n",
        "import torch\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "CooOF7NC-6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "e0DbA5-JTPm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Gather Text"
      ],
      "metadata": {
        "id": "Qn47PCNDAeWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = load_dataset(\"JeanKaddour/minipile\",split='train',trust_remote_code=True)"
      ],
      "metadata": {
        "id": "ss22Hz61gH2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = load_dataset('dvilasuero/distillama3-prompts10k', split='train')\n",
        "dataset3 = load_dataset(\"tatsu-lab/alpaca\", split = \"train\")\n",
        "dataset4 = load_dataset(\"garage-bAInd/Open-Platypus\", split = \"train\")"
      ],
      "metadata": {
        "id": "8LV1c6vxkr4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset5 = load_dataset(\"tatsu-lab/alpaca\", split = \"train\")\n",
        "# dataset6 = load_dataset(\"THUDM/webglm-qa\", split = \"train\")"
      ],
      "metadata": {
        "id": "LJJsYRxqq5Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = dataset2.rename_column('generations','output')\n",
        "dataset2 = dataset2.rename_column('generations','input')"
      ],
      "metadata": {
        "id": "Qq6fkOw7Mli0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset2),len(dataset3),len(dataset4)"
      ],
      "metadata": {
        "id": "BUpXBQ30JM1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training Tokenizer"
      ],
      "metadata": {
        "id": "geteU4Ri_t5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(path):\n",
        "    f =  open(path)\n",
        "    vocab = json.load(f)\n",
        "    f.close()\n",
        "    vocab = [i.encode('utf-8') for i in vocab]\n",
        "    return vocab\n",
        "\n",
        "def load_merges(path):\n",
        "    with open(path,'rb') as f:\n",
        "        merges = pickle.load(f)\n",
        "    return merges\n",
        "\n",
        "vocab = load_vocab(r'/content/drive/MyDrive/Pretrained_Models/vocab.json')\n",
        "merges = load_merges(r'/content/drive/MyDrive/Pretrained_Models/merges.pkl')"
      ],
      "metadata": {
        "id": "nV_xlXiegJ1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer():\n",
        "    def __init__(self, vocab=None, merges=None) -> None:\n",
        "        self.pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        self.compiled_pattern = re.compile(self.pattern)\n",
        "        self.vocab = vocab if vocab is not None else {}\n",
        "        self.merges = merges if merges is not None else {}\n",
        "\n",
        "    def get_stats(self, ids, counts=None):\n",
        "        counts = {} if counts is None else counts\n",
        "        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "                newids.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "        return newids\n",
        "\n",
        "    def train(self, text, vocab_size, verbose, merges_dir,  vocab_dir):\n",
        "        text = re.sub(r'[^\\w\\p{P}\\p{M}\\p{Z}}\"]+',' ',text)\n",
        "        num_merges = vocab_size - 256\n",
        "        text_chunks = re.findall(self.compiled_pattern, text)\n",
        "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for i in tqdm(range(num_merges),desc='Token merges'):\n",
        "            stats = {}\n",
        "            for chunk_ids in ids:\n",
        "                stats = self.get_stats(chunk_ids, stats)\n",
        "            if len(stats) > 0:\n",
        "                pair = max(stats, key=lambda x: stats[x])\n",
        "                idx = 256 + i\n",
        "                ids = [self.merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
        "                self.merges[pair] = idx\n",
        "                self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
        "                if verbose:\n",
        "                    print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({self.vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "            else:\n",
        "                pass\n",
        "        self.save_merges(merges_dir)\n",
        "        self.save_vocab(vocab_dir)\n",
        "        return self.merges, self.vocab\n",
        "\n",
        "    def encode_chunk(self,text_bytes):\n",
        "            # return the token ids\n",
        "            # let's begin. first, convert all bytes to integers in range 0..255\n",
        "            # text_bytes = bytes(text.encode('utf-8'))\n",
        "            ids = list(text_bytes)\n",
        "            while len(ids) >= 2:\n",
        "                # find the pair with the lowest merge index\n",
        "                stats = self.get_stats(ids)\n",
        "                pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "                # subtle: if there are no more merges available, the key will\n",
        "                # result in an inf for every single pair, and the min will be\n",
        "                # just the first pair in the list, arbitrarily\n",
        "                # we can detect this terminating case by a membership check\n",
        "                if pair not in self.merges:\n",
        "                    break # nothing else can be merged anymore\n",
        "                # otherwise let's merge the best pair (lowest merge index)\n",
        "                idx = self.merges[pair]\n",
        "                ids = self.merge(ids, pair, idx)\n",
        "            return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text_bytes = b\"\".join([self.vocab[i] for i in ids])\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def encode_ordinary(self, text):\n",
        "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
        "        # split text into chunks of text by categories defined in regex pattern\n",
        "        text_chunks = re.findall(self.compiled_pattern, text)\n",
        "        # all chunks of text are encoded separately, then results are joined\n",
        "        ids = []\n",
        "        for chunk in text_chunks:\n",
        "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
        "            chunk_ids = self.encode_chunk(chunk_bytes)\n",
        "            ids.extend(chunk_ids)\n",
        "        return ids\n",
        "\n",
        "    def save_vocab(self, save_path):\n",
        "        data = [i.decode('utf-8',errors='replace') for i in list(self.vocab.values())]\n",
        "        with open(save_path,'w') as file:\n",
        "            json.dump(data,file,indent=4)\n",
        "\n",
        "    def save_merges(self, save_path):\n",
        "        with open(save_path,'wb') as f:\n",
        "            pickle.dump(self.merges,f)\n",
        "\n",
        "def load_vocab(path):\n",
        "    f =  open(path)\n",
        "    vocab = json.load(f)\n",
        "    f.close()\n",
        "    vocab = [i.encode('utf-8') for i in vocab]\n",
        "    return vocab\n",
        "\n",
        "def load_merges(path):\n",
        "    with open(path,'rb') as f:\n",
        "        merges = pickle.load(f)\n",
        "    return merges\n"
      ],
      "metadata": {
        "id": "dCNZ1GJh-TyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HIgxrJdNpLYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_(dataset):\n",
        "    idx = random.randint(0,len(dataset)-1)\n",
        "    text = f\"Instruction:\\n {dataset['instruction']} \\nInput:\\n{dataset['input']} \\nOutput:\\n{dataset['instruction']}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "vlpNif1GKsxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = load_dataset_(dataset3)"
      ],
      "metadata": {
        "id": "ETLHQDHKKusU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BasicTokenizer(vocab,merges)"
      ],
      "metadata": {
        "id": "NLl5yXrs-YmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.train(text3, 900, False)"
      ],
      "metadata": {
        "id": "-w6gqh0H105j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "WAmcquyIOuZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler:\n",
        "    def __init__(self,tokenizer=None):\n",
        "        self.tokenizer = tokenizer if tokenizer is not None else CharacterTokenizer\n",
        "\n",
        "    def random_portion(self, text, chunk_size):\n",
        "        start_idx = random.randint(0,len(text) - chunk_size)\n",
        "        end_idx = start_idx + chunk_size + 1\n",
        "        text = text[start_idx: end_idx]\n",
        "        return text\n",
        "\n",
        "    def read_pdf(self,path):\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(path)\n",
        "        index = random.randint(0,len(pdf_reader.pages)-1)\n",
        "        for page in pdf_reader.pages[index:index+1]:\n",
        "            text += page.extract_text()\n",
        "        text = unidecode.unidecode(text)\n",
        "        return text\n",
        "\n",
        "    def preprocess(self,text):\n",
        "        text = unidecode.unidecode(text)\n",
        "        text = re.sub(r'[^\\p{L}\\p{N}\\p{M}\\p{Z}]+',' ',text)\n",
        "        return text\n",
        "\n",
        "    def generate_context_target_words(self, ids,seq_len):\n",
        "        context, targets = [], []\n",
        "        for i in range(seq_len,len(ids)-seq_len):\n",
        "            context.append(ids[i-seq_len:i])\n",
        "            targets.append(ids[i-seq_len+1:i+1])\n",
        "        return context, targets\n",
        "\n",
        "    def get_dataloaders(self,context,targets,batch_size,device):\n",
        "        X = torch.tensor(context,device=device)\n",
        "        y = torch.tensor(targets,device=device)\n",
        "        dataset = TensorDataset(X,y)\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_dataset, valid_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(dataset=train_dataset,shuffle=True,batch_size=batch_size)\n",
        "        valid_loader = DataLoader(dataset=valid_dataset,shuffle=True,batch_size=batch_size)\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "\n",
        "    def __call__(self, seq_len, batch_size, device,  text=None, pdf_path=None, chunk_size=1000):\n",
        "        if pdf_path is not None:\n",
        "            text = self.read_pdf(pdf_path)\n",
        "        else:\n",
        "            pass\n",
        "        text = text.lower()\n",
        "        text = self.random_portion(text, chunk_size)\n",
        "        text = self.preprocess(text)\n",
        "        ids = self.tokenizer.encode_text(text)\n",
        "        context, targets = self.generate_context_target_words(ids, seq_len)\n",
        "        train_loader, valid_loader = self.get_dataloaders(context, targets, batch_size, device)\n",
        "        return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "6i8Z_7JX_W4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Loading Model"
      ],
      "metadata": {
        "id": "DtZJrJf-iq2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "EXZJlFVgi4E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTokenizer:\n",
        "    def __init__(self, vocab = None) -> None:\n",
        "        self.vocab = vocab if vocab is not None else list(string.printable)\n",
        "        self.vocab = list(string.printable)\n",
        "        self.stoi = {s:i for i,s in enumerate(self.vocab)}\n",
        "        self.itos = {i:s for i,s in enumerate(self.vocab)}\n",
        "\n",
        "    def encode_text(self,text):\n",
        "        encoded = [self.stoi[words] for words in text]\n",
        "        return encoded\n",
        "\n",
        "    def decode_ids(self,ids):\n",
        "        decoded = ''.join([self.itos[idx] for idx in ids])\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "Fee3y_wlPWXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(string.ascii_lowercase + string.punctuation)\n",
        "tokenizer = CharacterTokenizer(vocab)\n",
        "sampler = Sampler(tokenizer)"
      ],
      "metadata": {
        "id": "PkEo2034qDPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    in_ = len(vocab)\n",
        "    batch = 128\n",
        "    seq_len = 128\n",
        "    dim = 256\n",
        "    q_heads = 8\n",
        "    kv_heads = 2\n",
        "    hdim = dim // q_heads\n",
        "    blocks = 4\n",
        "    dropout = 0.2"
      ],
      "metadata": {
        "id": "mzHkD7xMWtZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, n_dim, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(n_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_normed = (\n",
        "            x.float() * torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        ).type_as(x)\n",
        "        return x_normed * self.scale"
      ],
      "metadata": {
        "id": "pKezoUkuiXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    # SwiGLU(x,W,V,b,c) = x.sigmoid(BxW + b) * (xV+c), *-> element wise product\n",
        "    def __init__(self,n_dim):\n",
        "        super().__init__()\n",
        "        self.linear_gate= nn.Linear(n_dim, n_dim)\n",
        "        self.linear = nn.Linear(n_dim,n_dim)\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "        self.register_parameter('beta', self.beta)\n",
        "\n",
        "    def forward(self,x):\n",
        "        swish_gate = x * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "        out = swish_gate * self.linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "eTxgEjgfi6eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(seq_len, n_dim, base=10000):\n",
        "    R = torch.zeros((seq_len, n_dim, n_dim),requires_grad=False)\n",
        "    for p in range(seq_len):\n",
        "        for i in range(n_dim // 2):\n",
        "            theta = base ** (-2. * (i-1)/n_dim)\n",
        "            m_theta = torch.tensor(p * theta)\n",
        "            R[p, 2*i, 2*i] = torch.cos(m_theta)\n",
        "            R[p, 2*i, 2*i+1] = -torch.sin(m_theta)\n",
        "            R[p, 2*i+1, 2*i] = torch.sin(m_theta)\n",
        "            R[p, 2*i+1, 2*i+1] = torch.cos(m_theta)\n",
        "    return R"
      ],
      "metadata": {
        "id": "10wJ27sbi6WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "    def __init__(self, args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Linear(args.dim,args.dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim,args.dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim,args.dim, bias=False)\n",
        "        self.R = get_rotary_matrix(args.seq_len,args.dim)\n",
        "        self.drop = args.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "        self.R = self.R.to(x.device)\n",
        "        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:x.shape[1]])).transpose(0, 1)\n",
        "        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:x.shape[1]])).transpose(0, 1)\n",
        "        activations = F.scaled_dot_product_attention(q_rotated,k_rotated,v,dropout_p=self.drop,is_causal=True)\n",
        "        return activations"
      ],
      "metadata": {
        "id": "Nryk9-nOjJQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "    def __init__(self, args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([\n",
        "            RoPEMaskedAttentionHead(args) for _ in range(args.q_heads)\n",
        "        ])\n",
        "        self.linear = nn.Linear(args.q_heads*args.dim, args.dim)\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = [h(x) for h in self.heads]\n",
        "        x = torch.cat(heads, dim=-1)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BRP46BepjKQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelBlock(nn.Module):\n",
        "    def __init__(self, args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.rms = RMSNorm(args.dim)\n",
        "        self.rope_attention = RoPEMaskedMultiheadAttention(args)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(args.dim, args.dim),\n",
        "            SwiGLU(args.dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rms(x)\n",
        "        x = x + self.rope_attention(x)\n",
        "        x = self.rms(x)\n",
        "        x = x + self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "U9D_LYWLjPFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(args.in_,args.dim)\n",
        "        self.model_blocks = nn.Sequential(\n",
        "            OrderedDict([(f\"block_{i}\", ModelBlock(args)) for i in range(args.blocks)])\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(args.dim,args.dim),\n",
        "            SwiGLU(args.dim),\n",
        "            nn.Linear(args.dim,args.in_)\n",
        "        )\n",
        "    def forward(self, idx):\n",
        "        x = self.embeddings(idx)\n",
        "        x = self.model_blocks(x)\n",
        "        logits = self.ffn(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "QK8dDEEdjShH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Setup Model"
      ],
      "metadata": {
        "id": "vV0jMi4TldmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ytJsDm86pcgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainingArgs:\n",
        "    epochs = 1e3\n",
        "    lr = 1e-3\n",
        "    scheduler_gamma = 0.9999\n",
        "    use_mixed_precision = True\n",
        "    model_save_dir = r'/content/drive/MyDrive/Pretrained_Models/model_checkpoint_5.pt'\n",
        "    model_load_dir = model_save_dir\n",
        "    loss_dir = r'/content/drive/MyDrive/Pretrained_Models/loss.csv'"
      ],
      "metadata": {
        "id": "Tzho9e8gO62D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, save_path:str, epoch:int, scaler):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler':scaler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }, save_path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, scaler, load_path:str,device):\n",
        "    checkpoint = torch.load(load_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, scaler, epoch"
      ],
      "metadata": {
        "id": "sRjzY2L8jpAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_loss(loss_data,save_dir):\n",
        "    with open(save_dir, 'w') as f:\n",
        "        csvwriter = csv.writer(f)\n",
        "        csvwriter.writerows(loss_data)"
      ],
      "metadata": {
        "id": "FYQqJRH5PKml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, args:ModelArgs, text, train_args:TrainingArgs, dataloader, optimizer, scheduler, device, checkpointing=False, save_loss=False):\n",
        "    # Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    loss_logs = []\n",
        "    if checkpointing is True:\n",
        "        model, optimizer, scaler, epoch = load_checkpoint(model,optimizer,scaler, train_args.model_load_dir, device)\n",
        "    try:\n",
        "        model.train()\n",
        "        start = time.time()\n",
        "        for epoch in tqdm(range(int(train_args.epochs)),desc='Epochs',total=train_args.epochs,maxinterval=train_args.epochs//10):\n",
        "            train_dataloader, validation_dataloader = dataloader(args.seq_len, args.batch, device, text)\n",
        "            for x_batch, y_batch in train_dataloader:\n",
        "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=train_args.use_mixed_precision):\n",
        "                    y_pred = model(x_batch)\n",
        "                    train_loss = F.cross_entropy(y_pred.view(-1,y_pred.shape[2]),y_batch.view(-1))\n",
        "                scaler.scale(train_loss).backward()\n",
        "                # nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            model.eval()\n",
        "            with torch.inference_mode():\n",
        "                for x_batch, y_batch in validation_dataloader:\n",
        "                    y_pred = model(x_batch)\n",
        "                    validation_loss = F.cross_entropy(y_pred.view(-1,y_pred.shape[2]),y_batch.view(-1))\n",
        "\n",
        "            if epoch %  (train_args.epochs/10) == 0:\n",
        "                batch_time = time.time() - start\n",
        "                model.eval()\n",
        "                gpu_usage = round(torch.cuda.memory_reserved(0)/1024**3,1)\n",
        "                print(f'\\nEpoch: {epoch} | Train Loss : {train_loss.detach().item():.4f} | Eval Loss : {validation_loss.detach().item():.4f} | Time : {batch_time:.2f} s | Learning Rate: {scheduler.get_last_lr()[0]:.2e} | Gpu Usage: {gpu_usage}\\n')\n",
        "                model.train()\n",
        "            scheduler.step()\n",
        "            loss_logs.append((train_loss.detach().item(), validation_loss.detach().item()))\n",
        "    except KeyboardInterrupt:\n",
        "        save_checkpoint(model,optimizer,train_args.model_save_dir,epoch, scaler)\n",
        "\n",
        "    if TrainingArgs.model_save_dir is not None:\n",
        "        save_checkpoint(model,optimizer,train_args.model_save_dir,epoch, scaler)\n",
        "\n",
        "    if save_loss is True:\n",
        "        save_loss(loss_logs,TrainingArgs.loss_dir)"
      ],
      "metadata": {
        "id": "oDSTT5MYjSNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose_model(model, args:ModelArgs, device=None):\n",
        "    x = torch.randint(0,100,(args.batch,args.seq_len),device=device)\n",
        "    with profile(activities=[\n",
        "            ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "        with record_function(\"model_inference\"):\n",
        "            model(x)\n",
        "\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "    print('Number of Parameters in the Model:',sum(param.numel() for param in model.parameters()))"
      ],
      "metadata": {
        "id": "h-bJD9jZj-F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(ModelArgs)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "0fEpuWkgNy2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),TrainingArgs.lr)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)"
      ],
      "metadata": {
        "id": "kj_XBQ52k9Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Run Diagnostics"
      ],
      "metadata": {
        "id": "IMLNmFALlom6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagnose_model(model,ModelArgs,device)"
      ],
      "metadata": {
        "id": "rrE_BdIRlZyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Train Model"
      ],
      "metadata": {
        "id": "PYcMEhAol8bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    model=model,\n",
        "    args=ModelArgs,\n",
        "    text=text,\n",
        "    train_args=TrainingArgs,\n",
        "    dataloader=sampler,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    checkpointing=True,\n",
        "    save_loss=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "9A3lI-JUl7Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Inference"
      ],
      "metadata": {
        "id": "w-DNWjDFEbMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generate_greedy(model, input_ids, seq_len, gen_len, temperature):\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(gen_len):\n",
        "            X_trunc = input_ids[:,-seq_len:] # truncate all besides last context length terms\n",
        "            logits = model(X_trunc)\n",
        "            logits = logits / temperature # Scales the logits. Lower temperature makes higher logits peaky leading to higher confidence\n",
        "            logits = logits[:,-1,:] # use only last token for next token generation\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            next_tok = torch.multinomial(probs,num_samples=1)\n",
        "            input_ids = torch.cat((input_ids,next_tok),dim=1)\n",
        "    #gen = tokenizer.decode_ids(x[0].tolist())\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "eV46zNtlFN8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(input_ids, model, max_tokens=100, top_k=50, temperature=1.0):\n",
        "    for _ in range(max_tokens):\n",
        "        # Temporarily disables gradient calculation to improve performance and reduce memory usage\n",
        "        with torch.inference_mode():\n",
        "            logits = model(input_ids)\n",
        "            logits = logits[:, -1, :]\n",
        "            # Select the top K tokens from the probability distribution\n",
        "            top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities, with optional temperature scaling\n",
        "            top_k_probs = F.softmax(top_k_logits / temperature, dim=-1)\n",
        "\n",
        "            # Sample from the top K tokens to determine the next token\n",
        "            next_token_index = torch.multinomial(top_k_probs, num_samples=1)\n",
        "\n",
        "            # Map the sampled token back to its original index in the logits tensor\n",
        "            next_token = top_k_indices.gather(-1, next_token_index)\n",
        "\n",
        "            # Concatenate the new token to the sequence of input IDs\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "    return input_ids[0].tolist()"
      ],
      "metadata": {
        "id": "ta4GzlJJivSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(input_ids: list[int], model, temperature: float=1, max_tokens :int=100, top_p: float=0.95):\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(max_tokens):\n",
        "            logits = model(input_ids)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Sort the logits in descending order and apply softmax to get probabilities\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            sorted_probabilities = F.softmax(sorted_logits / temperature, dim=-1) # Apply temperature\n",
        "\n",
        "            # Calculate cumulative probabilities\n",
        "            cumulative_probs = torch.cumsum(sorted_probabilities, dim=-1)\n",
        "\n",
        "            # Identify and remove tokens with cumulative probability above the threshold (top_p)\n",
        "            # Ensuring the first token is always selected by setting the first position to False\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 0] = False  # Keep the most probable token\n",
        "\n",
        "            # Get the actual indices to remove from the original logits tensor\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "\n",
        "            # Set the logits of removed tokens to -infinity to exclude them from sampling\n",
        "            logits.scatter_(-1, indices_to_remove[None, :], float('-inf'))\n",
        "\n",
        "            # Re-calculate probabilities after filtering and sample from this distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the sampled token to the input IDs for the next iteration\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "    return input_ids[0].tolist()"
      ],
      "metadata": {
        "id": "MiXJptlWVGAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'What'\n",
        "input_ids = torch.tensor(tokenizer.encode_text(prompt),device=device).view(1,-1)\n",
        "decoded_ids = top_p_sampling(input_ids, model)\n",
        "print(tokenizer.decode_ids(decoded_ids))"
      ],
      "metadata": {
        "id": "QG6XaMlTFvrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5plHTb5bVLIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}